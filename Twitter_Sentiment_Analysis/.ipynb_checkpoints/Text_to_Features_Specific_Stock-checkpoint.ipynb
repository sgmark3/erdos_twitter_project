{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Features\n",
    "\n",
    "\n",
    "### Tl;dr for Frequent Users\n",
    "\n",
    "- The only lines to customize for each run are `localpath` and `company_name`, which are written [here](#dest3).\n",
    "- Run the [Libraries](#dest4) section, followed by [Helper Functions](#the_destination), then [Setup](#dest2) and all the remaining stuffs below it.\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, we load the table `tweets_data.csv` of Tweets that mention a particular company in the stock exchange. We compute the (normalized) number of words from various word libraries for each Tweet in the string format. The word libraries include:\n",
    "- `Henry08_poswords.txt` and `Henry08_negwords.txt`, containing positive and negative words, respectively, from Henry (2008).\n",
    "- `LM11_pos_words.txt` and `LM11_neg_words.txt`, containing words related to positive and negative sentiments, respectively, from Loughran and McDonald (2011).\n",
    "- `ML_positive_bigram.csv` and `ML_negative_bigram.csv`, containing positive and negative bigrams (no trigrams??????), respectively, from Hagenau et al. (2013). \n",
    "- `news_library.txt`, containing names of mainstream business news agencies.\n",
    "- Vader sentiment score using a function imported from our external file.\n",
    "\n",
    "The word counts will be added as new columns to the dataframe, and the final dataframe will be saved on a new file: `df_[companyname]_features_added.csv`. The file will be read into the notebook that performs our model fit.\n",
    "\n",
    "This notebook can easily be modified to treat a different tweet table file.\n",
    "\n",
    "The main purpose of this notebook is to walk through data preprocessing steps in a pedagogical fashion. The actual preprocessing work is executed in the notebook `Text_to_Features_Many_Stocks.ipynb`, which more conveniently loops the script of this notebook through all the companies.\n",
    "\n",
    "<a id='dest4'></a>\n",
    "### Libraries\n",
    "\n",
    "We use `pandas` for dataframe and `spaCy` for linguistic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `spaCy`'s `en_core_web_sm` model as the underlying English language processing model. Throughout this notebook, denote the model by `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dest2'></a>\n",
    "### Setup\n",
    "\n",
    "For the code in this section to work, one must render the functions in [Helper Functions](#the_destination) section first. The docstrings for helper functions also provide additional details about the method employed.\n",
    "<br><br>\n",
    "Define the local path of your repository folder.\n",
    "<a id='dest3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize based on your machine\n",
    "localpath = \"/Users/josht/Documents/GitHub/erdos_twitter_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file into dataframe `df_tweets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize based on which stock to work on\n",
    "company_name = \"Starbucks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(localpath + \"/data/df_\" + company_name + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet contents appear as strings in the `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities_cashtags</th>\n",
       "      <th>entities_hashtags</th>\n",
       "      <th>entities_urls</th>\n",
       "      <th>public_metrics_like_count</th>\n",
       "      <th>public_metrics_quote_count</th>\n",
       "      <th>public_metrics_reply_count</th>\n",
       "      <th>public_metrics_retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>entities_mentions</th>\n",
       "      <th>created_at_user</th>\n",
       "      <th>public_metrics_followers_count</th>\n",
       "      <th>public_metrics_following_count</th>\n",
       "      <th>public_metrics_listed_count</th>\n",
       "      <th>public_metrics_tweet_count</th>\n",
       "      <th>media_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-30 19:59:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Campus labor shortage delays opening of the St...</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-26 18:14:59</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-30 19:59:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Yo what are fire Starbucks drinks</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-05-23 16:28:47</td>\n",
       "      <td>96</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-30 19:59:07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-09-16 05:43:14</td>\n",
       "      <td>4151</td>\n",
       "      <td>4962</td>\n",
       "      <td>56</td>\n",
       "      <td>391400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at  entities_cashtags  entities_hashtags  entities_urls  \\\n",
       "0  2021-09-30 19:59:36                  0                  0              2   \n",
       "1  2021-09-30 19:59:12                  0                  0              0   \n",
       "2  2021-09-30 19:59:07                  0                  0              2   \n",
       "\n",
       "   public_metrics_like_count  public_metrics_quote_count  \\\n",
       "0                          4                           0   \n",
       "1                          4                           0   \n",
       "2                          0                           0   \n",
       "\n",
       "   public_metrics_reply_count  public_metrics_retweet_count  \\\n",
       "0                           0                             0   \n",
       "1                           3                             0   \n",
       "2                           0                             0   \n",
       "\n",
       "                                                text  entities_mentions  \\\n",
       "0  Campus labor shortage delays opening of the St...                  1   \n",
       "1                  Yo what are fire Starbucks drinks                  0   \n",
       "2  https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...                  2   \n",
       "\n",
       "       created_at_user  public_metrics_followers_count  \\\n",
       "0  2021-07-26 18:14:59                              21   \n",
       "1  2020-05-23 16:28:47                              96   \n",
       "2  2010-09-16 05:43:14                            4151   \n",
       "\n",
       "   public_metrics_following_count  public_metrics_listed_count  \\\n",
       "0                              28                            0   \n",
       "1                             116                            0   \n",
       "2                            4962                           56   \n",
       "\n",
       "   public_metrics_tweet_count  media_type  \n",
       "0                          35           0  \n",
       "1                         919           0  \n",
       "2                      391400           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the libraries of key phrases and news agencies names, then put them in a list called `keys`. Each element of `keys` is a set of words from the corresponding library file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_words = [localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_poswords.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_negwords.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/LM11_pos_words.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/LM11_neg_words.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_bigrams = [localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/ML_positive_bigram.csv\",\n",
    "                   localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/ML_negative_bigram.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_news = [localpath + \"/Twitter_Sentiment_Analysis/Relevance_Feature_Libraries/news_library.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All libraries\n",
    "keys = [get_keywords(keyfile) for keyfile in keyfiles_words] + [get_keybigrams(keyfile) for keyfile in keyfiles_bigrams] + [get_news_agencies(keyfile) for keyfile in keyfiles_news]\n",
    "\n",
    "# Ignoring bigrams (significantly faster)\n",
    "#keys = [get_keywords(keyfile) for keyfile in keyfiles_words] + [get_news_agencies(keyfile) for keyfile in keyfiles_news]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many words are there in each library, we compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 85, 354, 2355, 12130, 13330, 23]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(keys[i]) for i in range(len(keys))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the library names in a legible manner for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "key_library = [\"Henry08_pos\", \"Henry08_neg\", \"LM11_pos\", \"LM11_neg\", \n",
    "               \"Hagenau13_pos\", \"Hagenau13_neg\", \"News_agencies\"]\n",
    "\n",
    "# Run this during the trial run\n",
    "#key_library = [\"Henry08_pos\", \"Henry08_neg\", \"LM11_pos\", \"LM11_neg\", \"News_agencies\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts\n",
    "\n",
    "We apply `tweet_to_wordcounts` to each tweet in `df_tweets[\"text\"]`. Then, we store the results in `wordcounts_all`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_all = [[-1 for i in range(df_tweets.shape[0])] for j in range(len(keys))]\n",
    "\n",
    "for i in range(df_tweets.shape[0]):\n",
    "    \n",
    "    # Run this if ignoring bigrams\n",
    "    # wordcounts = tweet_to_wordcounts(df_tweets_shorten[\"text\"].iloc[i], keys)\n",
    "    \n",
    "    # Run this if including bigrams\n",
    "    wordcounts = tweet_to_wordcounts(df_tweets[\"text\"].iloc[i], keys[:4] + [keys[-1]])\n",
    "    bigramcounts = tweet_to_bigramcounts(df_tweets[\"text\"].iloc[i], keys[4:6])\n",
    "    \n",
    "    for j in range(len(keys)):\n",
    "        if j <= 3:\n",
    "            wordcounts_all[j][i] = wordcounts[j]\n",
    "        elif j == len(keys) - 1:\n",
    "            wordcounts_all[-1][i] = wordcounts[-1]\n",
    "        else:\n",
    "            wordcounts_all[j][i] = bigramcounts[j - 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we put the resulting word counts for each phrase library into the corresponding new column, e.g. `Word_count_Henry08_pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(keys[:-1])):\n",
    "    df_tweets[\"Word_count_\" + key_library[j]] = wordcounts_all[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the column for the number of news agency names that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"News_agencies_names_count\"] = wordcounts_all[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Score\n",
    "\n",
    "Now, we add an extra columns to the dataframe corresponding to the Vader scores. This part of the code is imported from `vader_tweet_sentiment.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vader_tweet_sentiment import vader_tweet_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = vader_tweet_sentiment(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Applying all the above operations related to word counts and mentioned stock indices, we modify `df_tweets` to the following form. Note that word counts are normalized, i.e. divided, by the total word count for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Word_count_Henry08_pos</th>\n",
       "      <th>Word_count_Henry08_neg</th>\n",
       "      <th>Word_count_LM11_pos</th>\n",
       "      <th>Word_count_LM11_neg</th>\n",
       "      <th>Word_count_Hagenau13_pos</th>\n",
       "      <th>Word_count_Hagenau13_neg</th>\n",
       "      <th>News_agencies_names_count</th>\n",
       "      <th>Compound_vader</th>\n",
       "      <th>Positive_vader</th>\n",
       "      <th>Negative_vader</th>\n",
       "      <th>Neutral_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Campus labor shortage delays opening of the St...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yo what are fire Starbucks drinks</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3716</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Craving Auntie Anne's and Starbucks most deffüò≠...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love spoken word music. It‚Äôs so ridiculous. ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9002</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36502</th>\n",
       "      <td>At starbucks getting my morning coffee..almost...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36503</th>\n",
       "      <td>Happy Friday! Coffee is the fuel of humans üòπ\\n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7840</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36504</th>\n",
       "      <td>Freedom has always came at a great cost! Remem...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36505</th>\n",
       "      <td>Ten gunning for the Starbucks sponsorship, est...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36506</th>\n",
       "      <td>mothers obsessed w pumpkin spice were @ starbu...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36507 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      Campus labor shortage delays opening of the St...   \n",
       "1                      Yo what are fire Starbucks drinks   \n",
       "2      https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...   \n",
       "3      Craving Auntie Anne's and Starbucks most deffüò≠...   \n",
       "4      I love spoken word music. It‚Äôs so ridiculous. ...   \n",
       "...                                                  ...   \n",
       "36502  At starbucks getting my morning coffee..almost...   \n",
       "36503  Happy Friday! Coffee is the fuel of humans üòπ\\n...   \n",
       "36504  Freedom has always came at a great cost! Remem...   \n",
       "36505  Ten gunning for the Starbucks sponsorship, est...   \n",
       "36506  mothers obsessed w pumpkin spice were @ starbu...   \n",
       "\n",
       "       Word_count_Henry08_pos  Word_count_Henry08_neg  Word_count_LM11_pos  \\\n",
       "0                    0.000000                     0.0             0.000000   \n",
       "1                    0.000000                     0.0             0.000000   \n",
       "2                    0.000000                     0.0             0.005348   \n",
       "3                    0.014286                     0.0             0.000000   \n",
       "4                    0.000000                     0.0             0.000000   \n",
       "...                       ...                     ...                  ...   \n",
       "36502                0.000000                     0.0             0.000000   \n",
       "36503                0.000000                     0.0             0.006173   \n",
       "36504                0.000000                     0.0             0.004505   \n",
       "36505                0.000000                     0.0             0.000000   \n",
       "36506                0.000000                     0.0             0.000000   \n",
       "\n",
       "       Word_count_LM11_neg  Word_count_Hagenau13_pos  \\\n",
       "0                 0.012579                       0.0   \n",
       "1                 0.000000                       0.0   \n",
       "2                 0.000000                       0.0   \n",
       "3                 0.000000                       0.0   \n",
       "4                 0.000000                       0.0   \n",
       "...                    ...                       ...   \n",
       "36502             0.000000                       0.0   \n",
       "36503             0.000000                       0.0   \n",
       "36504             0.004505                       0.0   \n",
       "36505             0.000000                       0.0   \n",
       "36506             0.000000                       0.0   \n",
       "\n",
       "       Word_count_Hagenau13_neg  News_agencies_names_count  Compound_vader  \\\n",
       "0                           0.0                        0.0         -0.2500   \n",
       "1                           0.0                        0.0         -0.3400   \n",
       "2                           0.0                        0.0          0.3716   \n",
       "3                           0.0                        0.0         -0.5209   \n",
       "4                           0.0                        0.0          0.9002   \n",
       "...                         ...                        ...             ...   \n",
       "36502                       0.0                        0.0          0.0000   \n",
       "36503                       0.0                        0.0          0.7840   \n",
       "36504                       0.0                        0.0          0.9258   \n",
       "36505                       0.0                        0.0          0.0000   \n",
       "36506                       0.0                        0.0          0.2500   \n",
       "\n",
       "       Positive_vader  Negative_vader  Neutral_vader  \n",
       "0               0.000           0.095          0.905  \n",
       "1               0.000           0.324          0.676  \n",
       "2               0.192           0.084          0.724  \n",
       "3               0.000           0.252          0.748  \n",
       "4               0.421           0.103          0.476  \n",
       "...               ...             ...            ...  \n",
       "36502           0.000           0.000          1.000  \n",
       "36503           0.246           0.060          0.694  \n",
       "36504           0.352           0.132          0.516  \n",
       "36505           0.000           0.000          1.000  \n",
       "36506           0.237           0.149          0.614  \n",
       "\n",
       "[36507 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[[\"text\", \"Word_count_Henry08_pos\", \"Word_count_Henry08_neg\", \"Word_count_LM11_pos\", \"Word_count_LM11_neg\", \n",
    "           \"Word_count_Hagenau13_pos\", \"Word_count_Hagenau13_neg\",\"News_agencies_names_count\", \"Compound_vader\", \n",
    "           \"Positive_vader\", \"Negative_vader\", \"Neutral_vader\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we save the result into a file, we delete all the rows with zero word counts from all dictionaries and absolutely neutral Vader score. The latter means zero positive, negative and compound Vader scores, with neutral Vader score of 1. Altogether, we remove 20-25% of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trivial_tweets = df_tweets.loc[df_tweets[\"Word_count_Henry08_pos\"] == 0].loc[df_tweets[\"Word_count_Henry08_neg\"] == 0].loc[df_tweets[\"Word_count_LM11_pos\"] == 0].loc[df_tweets[\"Word_count_LM11_neg\"] == 0].loc[df_tweets[\"Word_count_Hagenau13_pos\"] == 0].loc[df_tweets[\"Word_count_Hagenau13_neg\"] == 0].loc[df_tweets[\"News_agencies_names_count\"] == 0].loc[df_tweets[\"Compound_vader\"] == 0].loc[df_tweets[\"Positive_vader\"] == 0].loc[df_tweets[\"Negative_vader\"] == 0].loc[df_tweets[\"Neutral_vader\"] == 1]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_shorten = df_tweets.drop(df_trivial_tweets.index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Word_count_Henry08_pos</th>\n",
       "      <th>Word_count_Henry08_neg</th>\n",
       "      <th>Word_count_LM11_pos</th>\n",
       "      <th>Word_count_LM11_neg</th>\n",
       "      <th>Word_count_Hagenau13_pos</th>\n",
       "      <th>Word_count_Hagenau13_neg</th>\n",
       "      <th>News_agencies_names_count</th>\n",
       "      <th>Compound_vader</th>\n",
       "      <th>Positive_vader</th>\n",
       "      <th>Negative_vader</th>\n",
       "      <th>Neutral_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Campus labor shortage delays opening of the St...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yo what are fire Starbucks drinks</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3716</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Craving Auntie Anne's and Starbucks most deffüò≠...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love spoken word music. It‚Äôs so ridiculous. ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9002</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36500</th>\n",
       "      <td>Today‚Äôs Starbucks drink üëçüí´üí´üßãüßã https://t.co/eS2...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36501</th>\n",
       "      <td>Macmillan coffee morning has arrived in Drakes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36503</th>\n",
       "      <td>Happy Friday! Coffee is the fuel of humans üòπ\\n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7840</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36504</th>\n",
       "      <td>Freedom has always came at a great cost! Remem...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36506</th>\n",
       "      <td>mothers obsessed w pumpkin spice were @ starbu...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28065 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      Campus labor shortage delays opening of the St...   \n",
       "1                      Yo what are fire Starbucks drinks   \n",
       "2      https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...   \n",
       "3      Craving Auntie Anne's and Starbucks most deffüò≠...   \n",
       "4      I love spoken word music. It‚Äôs so ridiculous. ...   \n",
       "...                                                  ...   \n",
       "36500  Today‚Äôs Starbucks drink üëçüí´üí´üßãüßã https://t.co/eS2...   \n",
       "36501  Macmillan coffee morning has arrived in Drakes...   \n",
       "36503  Happy Friday! Coffee is the fuel of humans üòπ\\n...   \n",
       "36504  Freedom has always came at a great cost! Remem...   \n",
       "36506  mothers obsessed w pumpkin spice were @ starbu...   \n",
       "\n",
       "       Word_count_Henry08_pos  Word_count_Henry08_neg  Word_count_LM11_pos  \\\n",
       "0                    0.000000                     0.0             0.000000   \n",
       "1                    0.000000                     0.0             0.000000   \n",
       "2                    0.000000                     0.0             0.005348   \n",
       "3                    0.014286                     0.0             0.000000   \n",
       "4                    0.000000                     0.0             0.000000   \n",
       "...                       ...                     ...                  ...   \n",
       "36500                0.000000                     0.0             0.000000   \n",
       "36501                0.000000                     0.0             0.000000   \n",
       "36503                0.000000                     0.0             0.006173   \n",
       "36504                0.000000                     0.0             0.004505   \n",
       "36506                0.000000                     0.0             0.000000   \n",
       "\n",
       "       Word_count_LM11_neg  Word_count_Hagenau13_pos  \\\n",
       "0                 0.012579                       0.0   \n",
       "1                 0.000000                       0.0   \n",
       "2                 0.000000                       0.0   \n",
       "3                 0.000000                       0.0   \n",
       "4                 0.000000                       0.0   \n",
       "...                    ...                       ...   \n",
       "36500             0.000000                       0.0   \n",
       "36501             0.000000                       0.0   \n",
       "36503             0.000000                       0.0   \n",
       "36504             0.004505                       0.0   \n",
       "36506             0.000000                       0.0   \n",
       "\n",
       "       Word_count_Hagenau13_neg  News_agencies_names_count  Compound_vader  \\\n",
       "0                           0.0                        0.0         -0.2500   \n",
       "1                           0.0                        0.0         -0.3400   \n",
       "2                           0.0                        0.0          0.3716   \n",
       "3                           0.0                        0.0         -0.5209   \n",
       "4                           0.0                        0.0          0.9002   \n",
       "...                         ...                        ...             ...   \n",
       "36500                       0.0                        0.0         -0.2263   \n",
       "36501                       0.0                        0.0          0.6597   \n",
       "36503                       0.0                        0.0          0.7840   \n",
       "36504                       0.0                        0.0          0.9258   \n",
       "36506                       0.0                        0.0          0.2500   \n",
       "\n",
       "       Positive_vader  Negative_vader  Neutral_vader  \n",
       "0               0.000           0.095          0.905  \n",
       "1               0.000           0.324          0.676  \n",
       "2               0.192           0.084          0.724  \n",
       "3               0.000           0.252          0.748  \n",
       "4               0.421           0.103          0.476  \n",
       "...               ...             ...            ...  \n",
       "36500           0.000           0.213          0.787  \n",
       "36501           0.172           0.000          0.828  \n",
       "36503           0.246           0.060          0.694  \n",
       "36504           0.352           0.132          0.516  \n",
       "36506           0.237           0.149          0.614  \n",
       "\n",
       "[28065 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_shorten[[\"text\", \"Word_count_Henry08_pos\", \"Word_count_Henry08_neg\", \"Word_count_LM11_pos\", \"Word_count_LM11_neg\", \n",
    "           \"Word_count_Hagenau13_pos\", \"Word_count_Hagenau13_neg\",\"News_agencies_names_count\", \"Compound_vader\", \n",
    "           \"Positive_vader\", \"Negative_vader\", \"Neutral_vader\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the new `df_tweets_shorten` onto a new csv file called `df_[companyname]_features_added.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining the text\n",
    "df_tweets_shorten.to_csv(localpath + \"/Data_Preprocessed/df_\" + company_name + \"_features_added_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the text to save space further\n",
    "df_tweets_notext = df_tweets_shorten.drop(columns=[\"text\"]).copy()\n",
    "\n",
    "df_tweets_notext.to_csv(localpath + \"/Data_Preprocessed/df_\" + company_name + \"_features_added.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of the text itself reduces to file size to about 50% the size with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>\n",
    "### Helper Functions\n",
    "\n",
    "For brevity, we write down all the necessary but lengthy functions in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains keywords separated by space.\n",
    "                \n",
    "    Output: \n",
    "    keywords -> The set of strings, each of which is a word from the txt file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define keywords\n",
    "    keywords = []\n",
    "    \n",
    "    # This is to keep track of the word we are reading as we traverse text.\n",
    "    this_word = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":    # When running into \" \", we have finished reading a word. \n",
    "            keywords.append(this_word)\n",
    "            this_word = \"\"\n",
    "        elif text[i:] == \"\\n\":   # This may occur at the end of the string.\n",
    "            break\n",
    "        else:     # With an additional letter, just add it to the current word.\n",
    "            this_word = this_word + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \" \", we will need to append the last word to keywords.\n",
    "    if this_word != \"\":\n",
    "        keywords.append(this_word)\n",
    "    \n",
    "    # Return the result in the set format.\n",
    "    return set(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keybigrams(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains key bigrams separated by \"\\n\". \n",
    "    \n",
    "    This function should work for any n-grams, given that the text file is written in the same format.\n",
    "                \n",
    "    Output: \n",
    "    keybigrams_lemm -> The set of strings, each of which is a bigram from the csv file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define keywords\n",
    "    keybigrams = []\n",
    "    \n",
    "    # This is to keep track of the word we are reading as we traverse text.\n",
    "    this_bigram = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \"\\n\":    # When running into \"\\n\", we have finished reading a bigram.\n",
    "            keybigrams.append(this_bigram)\n",
    "            this_bigram = \"\"\n",
    "        else:     # With an additional letter or space, just add it to the current bigram.\n",
    "            this_bigram = this_bigram + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \"\\n\", we will need to append the last bigram to keybigrams.\n",
    "    if this_bigram != \"\":\n",
    "        keybigrams.append(this_bigram)\n",
    "    \n",
    "    # Return the result in the set format.\n",
    "    return set(keybigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_agencies(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains names of news agencies separated by space.\n",
    "                \n",
    "    Output: \n",
    "    news_agencies -> The set of strings, each of which is a name of news agency from the input file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define news_agencies\n",
    "    news_agencies = []\n",
    "    \n",
    "    # This is to keep track of the news agency name we are reading as we traverse text.\n",
    "    this_word = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":    # When running into \" \", we have finished reading an agency's name. \n",
    "            news_agencies.append(this_word)\n",
    "            this_word = \"\"\n",
    "        elif text[i:] == \"\\n\":   # This may occur at the end of the string.\n",
    "            break\n",
    "        else:     # With an additional letter, just add it to the current agency's name.\n",
    "            this_word = this_word + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \" \", we will need to append the last agency's name to news_agencies.\n",
    "    if this_word != \"\":\n",
    "        news_agencies.append(this_word)\n",
    "    \n",
    "    # Return the result in the set format.\n",
    "    return set(news_agencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_wordcounts(tweet, keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    keys -> The list of sets of key words. For example, keys = [henry08_pos, henry08_neg, ..., newslib]\n",
    "            Each keyword is assumed to contain only English letter. WARNING: must remove bigrams\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define a spaCy's doc object for the tweet\n",
    "    tweet_doc = nlp(tweet.lower())\n",
    "    \n",
    "    # Convert the doc object into a set of words\n",
    "    tweet_words = set([token.text for token in tweet_doc])\n",
    "    \n",
    "    # Initialize wordcounts\n",
    "    wordcounts = []\n",
    "    \n",
    "    # For each words library, we count the number of words in tweet using the more efficient \n",
    "    # intersection method. Then, if called for, we normalize the count by the length of the raw tweet.\n",
    "    for i in range(len(keys)):      # Not including the news agencies for now\n",
    "        this_wordcount = len(tweet_words.intersection(keys[i])) \n",
    "        if normalize:\n",
    "            this_wordcount_normalized = this_wordcount / len(tweet)\n",
    "            wordcounts.append(this_wordcount_normalized)\n",
    "        else:\n",
    "            wordcounts.append(this_wordcount)\n",
    "    \n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_bigramcounts(tweet, keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    keys -> The list of sets of key bigrams. For example, keys = [\"Hagenau13_pos\", \"Hagenau13_neg\"]\n",
    "            Each key bigram is assumed to contain only English letter and space.\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define a spaCy's doc object for the tweet\n",
    "    tweet_doc = nlp(tweet.lower())\n",
    "        \n",
    "    # Convert the doc object into a list of words with stop words removed, in accordance with the bigram libraries.\n",
    "    tweet_words = [token.text for token in tweet_doc if not token.is_stop]\n",
    "    \n",
    "    # Define the set of bigrams from the tweet, consisting of pairs of neighboring words.\n",
    "    tweet_bigrams = set([tweet_words[i] + \" \" + tweet_words[i+1] for i in range(len(tweet_words) - 1)])\n",
    "    \n",
    "    # Initialize wordcounts\n",
    "    wordcounts = []\n",
    "    \n",
    "    # For each bigrams library, we count the number of bigrams in tweet_bigrams using the more efficient \n",
    "    # intersection method. Then, if called for, we normalize the count by the length of the raw tweet.\n",
    "    for i in range(len(keys)):      # Not including the news agencies for now\n",
    "        this_wordcount = len(tweet_bigrams.intersection(keys[i])) \n",
    "        if normalize:\n",
    "            this_wordcount_normalized = this_wordcount / len(tweet)\n",
    "            wordcounts.append(this_wordcount_normalized)\n",
    "        else:\n",
    "            wordcounts.append(this_wordcount)\n",
    "    \n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the user finish running all these helper functions, jump back to the [Setup](#dest2) section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
