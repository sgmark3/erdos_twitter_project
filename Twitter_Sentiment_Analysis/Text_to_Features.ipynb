{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Features\n",
    "\n",
    "### Current Issues\n",
    "\n",
    "The list of stock indices given in the files `snp500_list.csv` and `nyse_list.csv` contain some indices with single-letter symbols, e.g. `A` for Agilent Technologies. This makes identifying mentions of these indices in tweets seemingly impossible. My solutions would be:\n",
    "- Check if the indices are extracted correctly. \n",
    "- See if there is a standard convention of how stock indices are mentioned in Twitter text, e.g. some tweets use `$` in front of stock indices.\n",
    "- As a last resort, we may toss these stocks from our analysis.\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, we load the table `tweets_data.csv` of Tweets. We compute the (normalized) number of words from various word libraries for each Tweet in the string format. The word libraries include:\n",
    "- `Henry08_poswords.txt` and `Henry08_negwords.txt`, containing positive and negative words, respectively, from Henry (2008).\n",
    "- `LM11_pos_words.txt` and `LM11_neg_words.txt`, containing words related to positive and negative sentiments, respectively, from Loughran and McDonald (2011).\n",
    "- `ML_positive_bigram.csv` and `ML_negative_bigram.csv`, containing positive and negative bigrams (no trigrams??????), respectively, from Hagenau et al. (2013). \n",
    "- `news_library.txt`, containing names of mainstream business news agencies.\n",
    "Furthermore, we identify the stock indices listed in `snp500_list.csv` and `nyse_list.csv` for each row in the table.\n",
    "\n",
    "The word counts and the list of mentioned stock indices will be added as new columns to the dataframe and will be saved on a new file: `tweets_data_features_added.csv`. The file will be read into the notebook that performs our model fit.\n",
    "\n",
    "This notebook can easily be modified to treat a different tweet table file.\n",
    "\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We use `pandas` for dataframe and `spaCy` for linguistic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `spaCy`'s `en_core_web_sm` model as the underlying English language processing model. Throughout this notebook, denote the model by `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we use `PhraseMatcher` (https://spacy.io/api/phrasematcher) to find word counts in order to conveniently work with bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "For the code in this section to work, one must render the functions in [Helper Functions](#the_destination) section first. The docstrings for helper functions also provide additional details about the method employed.\n",
    "<br><br>\n",
    "Define the local path of your repository folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "localpath = \"/Users/josht/Documents/GitHub/erdos_twitter_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file `tweets_data.csv` into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "#df_tweets = pd.read_csv(localpath + \"/data/tweets_data.csv\")\n",
    "\n",
    "# Run this during the trial run\n",
    "df_tweets = pd.read_csv(localpath + \"/data/Tweets_Raw/df_tsla OR aapl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet contents appear as strings in the `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_at_user</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>public_metrics_followers_count</th>\n",
       "      <th>public_metrics_following_count</th>\n",
       "      <th>public_metrics_like_count</th>\n",
       "      <th>public_metrics_listed_count</th>\n",
       "      <th>public_metrics_quote_count</th>\n",
       "      <th>public_metrics_reply_count</th>\n",
       "      <th>public_metrics_retweet_count</th>\n",
       "      <th>public_metrics_tweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187896454</td>\n",
       "      <td>2018-12-31T23:59:31.000Z</td>\n",
       "      <td>2010-09-07T12:49:16.000Z</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>lui toth</td>\n",
       "      <td>39249</td>\n",
       "      <td>42551</td>\n",
       "      <td>2</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30125</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>$BLSP huge volume, closes up 12.5%.  Shares st...</td>\n",
       "      <td>1079889823161303047</td>\n",
       "      <td>sprtcrdlui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19593572</td>\n",
       "      <td>2018-12-31T23:58:57.000Z</td>\n",
       "      <td>2009-01-27T14:21:12.000Z</td>\n",
       "      <td>Denver/Colorado/Everywhere</td>\n",
       "      <td>Technoking Tim</td>\n",
       "      <td>25631</td>\n",
       "      <td>26385</td>\n",
       "      <td>0</td>\n",
       "      <td>577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>79787</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @Polixenes13: Ross, please just never stop ...</td>\n",
       "      <td>1079889679774871552</td>\n",
       "      <td>TimWJackson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>996125781620330500</td>\n",
       "      <td>2018-12-31T23:58:40.000Z</td>\n",
       "      <td>2018-05-14T20:31:08.000Z</td>\n",
       "      <td>Georgia, USA</td>\n",
       "      <td>TeslaFamily</td>\n",
       "      <td>148</td>\n",
       "      <td>195</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>931</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@kimpaquette I pity the fool that is shorting ...</td>\n",
       "      <td>1079889607515426816</td>\n",
       "      <td>Tesla90801351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id                created_at           created_at_user  \\\n",
       "0           187896454  2018-12-31T23:59:31.000Z  2010-09-07T12:49:16.000Z   \n",
       "1            19593572  2018-12-31T23:58:57.000Z  2009-01-27T14:21:12.000Z   \n",
       "2  996125781620330500  2018-12-31T23:58:40.000Z  2018-05-14T20:31:08.000Z   \n",
       "\n",
       "                     location            name  public_metrics_followers_count  \\\n",
       "0                   Louisiana        lui toth                           39249   \n",
       "1  Denver/Colorado/Everywhere  Technoking Tim                           25631   \n",
       "2                Georgia, USA     TeslaFamily                             148   \n",
       "\n",
       "   public_metrics_following_count  public_metrics_like_count  \\\n",
       "0                           42551                          2   \n",
       "1                           26385                          0   \n",
       "2                             195                          2   \n",
       "\n",
       "   public_metrics_listed_count  public_metrics_quote_count  \\\n",
       "0                          298                           0   \n",
       "1                          577                           0   \n",
       "2                            4                           0   \n",
       "\n",
       "   public_metrics_reply_count  public_metrics_retweet_count  \\\n",
       "0                           0                             1   \n",
       "1                           0                             2   \n",
       "2                           0                             0   \n",
       "\n",
       "   public_metrics_tweet_count               source  \\\n",
       "0                       30125   Twitter Web Client   \n",
       "1                       79787   Twitter for iPhone   \n",
       "2                         931  Twitter for Android   \n",
       "\n",
       "                                                text             tweet_id  \\\n",
       "0  $BLSP huge volume, closes up 12.5%.  Shares st...  1079889823161303047   \n",
       "1  RT @Polixenes13: Ross, please just never stop ...  1079889679774871552   \n",
       "2  @kimpaquette I pity the fool that is shorting ...  1079889607515426816   \n",
       "\n",
       "        username  \n",
       "0     sprtcrdlui  \n",
       "1    TimWJackson  \n",
       "2  Tesla90801351  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the libraries of key phrases and put them in a list called `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_words = [localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_poswords.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_negwords.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/LM11_pos_words.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/LM11_neg_words.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_bigrams = [localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/ML_positive_bigram.csv\",\n",
    "                   localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/ML_negative_bigram.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "#keys = [get_keywords(keyfile) for keyfile in keyfiles_words] + [get_keybigrams(keyfile) for keyfile in keyfiles_bigrams]\n",
    "\n",
    "# Run this during the trial run\n",
    "keys = [get_keywords(keyfile) for keyfile in keyfiles_words[:4]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many words are there in each library, we compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 40, 225, 1285]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(keys[i]) for i in range(len(keys))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the library names in a legible manner for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "#key_library = [\"Henry08_pos\", \"Henry08_neg\", \"LM11_pos\", \"LM11_neg\", \"Hagenau13_pos\", \"Hagenau13_neg\"]\n",
    "\n",
    "# Run this during the trial run\n",
    "key_library = [\"Henry08_pos\", \"Henry08_neg\", \"LM11_pos\", \"LM11_neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare the list of stock indices, starting once again from the file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_stocks = [localpath + \"/data/Stock_indices/snp500_list.csv\",\n",
    "                  localpath + \"/data/Stock_indices/nyse_list.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataframe from each csv file, then extract the list of stock indices acronyms from the dataframe. Finally, we store all lists of indices into `stocks`, in a similar manner to `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "\n",
    "for file in keyfiles_stocks:\n",
    "    df_stocks = pd.read_csv(file)\n",
    "    stocks.append(list(df_stocks[\"Symbol\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `snp500_list.csv`, the company names are available under column `Security`. However, for `nyse_list.csv`, only index names are available under column `Name`. Unfortunately, there is no regular pattern to go from index names to company names. Also, index names are so long and specific that it is probably very rarely mentioned in full on Twitter. However, I am open to an alternative solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = []\n",
    "\n",
    "# Add company names from snp500_list.csv\n",
    "df_stocks = pd.read_csv(keyfiles_stocks[0])\n",
    "company_names.append(list(df_stocks[\"Security\"]))\n",
    "\n",
    "# As a place holder for nyse_list.csv, make it an empty list\n",
    "company_names.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the stock index library names for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_library = [\"S&P500\", \"NYSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentioned Stock Indices\n",
    "\n",
    "We apply `get_stock_list` to each tweet in `df_tweets[\"text\"]` and each stock library in `stocks`. We store the stock lists in the list called `mentioned_stocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_stocks = [[[] for i in range(df_tweets.shape[0])] for j in range(len(stocks))]\n",
    "\n",
    "for j in range(len(stocks)):\n",
    "    for i in range(df_tweets.shape[0]):\n",
    "        mentioned_stocks[j][i] = get_stock_list(df_tweets[\"text\"].iloc[i], stocks[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we put `mentioned_stocks` into corresponding new columns, e.g. `Mentioned_stocks_S&P500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(stocks)):\n",
    "    df_tweets[\"Mentioned_stocks_\" + stock_library[j]] = mentioned_stocks[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that new columns are added listing the mentioned stocks. Some rows have empty lists for both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_at_user</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>public_metrics_followers_count</th>\n",
       "      <th>public_metrics_following_count</th>\n",
       "      <th>public_metrics_like_count</th>\n",
       "      <th>public_metrics_listed_count</th>\n",
       "      <th>public_metrics_quote_count</th>\n",
       "      <th>public_metrics_reply_count</th>\n",
       "      <th>public_metrics_retweet_count</th>\n",
       "      <th>public_metrics_tweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>Mentioned_stocks_S&amp;P500</th>\n",
       "      <th>Mentioned_stocks_NYSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26440443</td>\n",
       "      <td>2018-12-31T23:44:18.000Z</td>\n",
       "      <td>2009-03-25T05:42:31.000Z</td>\n",
       "      <td>United States</td>\n",
       "      <td>Shannon Powers</td>\n",
       "      <td>82</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2030</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>RT @Hein_The_Sayer: @RationalEtienne @rm_natas...</td>\n",
       "      <td>1079885991739940864</td>\n",
       "      <td>rosepaleblue</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>996125781620330500</td>\n",
       "      <td>2018-12-31T23:58:40.000Z</td>\n",
       "      <td>2018-05-14T20:31:08.000Z</td>\n",
       "      <td>Georgia, USA</td>\n",
       "      <td>TeslaFamily</td>\n",
       "      <td>148</td>\n",
       "      <td>195</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>931</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@kimpaquette I pity the fool that is shorting ...</td>\n",
       "      <td>1079889607515426816</td>\n",
       "      <td>Tesla90801351</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1214284580</td>\n",
       "      <td>2018-12-31T23:43:57.000Z</td>\n",
       "      <td>2013-02-24T02:46:06.000Z</td>\n",
       "      <td>Durban, South Africa ðŸ‡¿ðŸ‡¦</td>\n",
       "      <td>Vaccinated Renaissance ManðŸ’‰ðŸ˜·</td>\n",
       "      <td>2547</td>\n",
       "      <td>1331</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53716</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@RationalEtienne @rm_natasha @TradrFloridaFIL ...</td>\n",
       "      <td>1079885905920438272</td>\n",
       "      <td>Hein_The_Slayer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author_id                created_at           created_at_user  \\\n",
       "13            26440443  2018-12-31T23:44:18.000Z  2009-03-25T05:42:31.000Z   \n",
       "2   996125781620330500  2018-12-31T23:58:40.000Z  2018-05-14T20:31:08.000Z   \n",
       "14          1214284580  2018-12-31T23:43:57.000Z  2013-02-24T02:46:06.000Z   \n",
       "\n",
       "                   location                          name  \\\n",
       "13            United States                Shannon Powers   \n",
       "2              Georgia, USA                   TeslaFamily   \n",
       "14  Durban, South Africa ðŸ‡¿ðŸ‡¦  Vaccinated Renaissance ManðŸ’‰ðŸ˜·   \n",
       "\n",
       "    public_metrics_followers_count  public_metrics_following_count  \\\n",
       "13                              82                             225   \n",
       "2                              148                             195   \n",
       "14                            2547                            1331   \n",
       "\n",
       "    public_metrics_like_count  public_metrics_listed_count  \\\n",
       "13                          0                            0   \n",
       "2                           2                            4   \n",
       "14                          2                           15   \n",
       "\n",
       "    public_metrics_quote_count  public_metrics_reply_count  \\\n",
       "13                           0                           0   \n",
       "2                            0                           0   \n",
       "14                           0                           0   \n",
       "\n",
       "    public_metrics_retweet_count  public_metrics_tweet_count  \\\n",
       "13                             3                        2030   \n",
       "2                              0                         931   \n",
       "14                             0                       53716   \n",
       "\n",
       "                 source                                               text  \\\n",
       "13  Twitter for Android  RT @Hein_The_Sayer: @RationalEtienne @rm_natas...   \n",
       "2   Twitter for Android  @kimpaquette I pity the fool that is shorting ...   \n",
       "14  Twitter for Android  @RationalEtienne @rm_natasha @TradrFloridaFIL ...   \n",
       "\n",
       "               tweet_id         username Mentioned_stocks_S&P500  \\\n",
       "13  1079885991739940864     rosepaleblue                      []   \n",
       "2   1079889607515426816    Tesla90801351                  [TSLA]   \n",
       "14  1079885905920438272  Hein_The_Slayer                      []   \n",
       "\n",
       "   Mentioned_stocks_NYSE  \n",
       "13                    []  \n",
       "2                     []  \n",
       "14                    []  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to save time for future steps, we drop from `df_tweets` the rows whose tweet mentions no stock index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the indices for the rows in which no stock from each stock library is mentioned.\n",
    "emp_ind = []\n",
    "for i in range(df_tweets.shape[0]):\n",
    "    if len(df_tweets[\"Mentioned_stocks_S&P500\"].iloc[i]) == 0 and len(df_tweets[\"Mentioned_stocks_NYSE\"].iloc[i]) == 0:\n",
    "        emp_ind.append(i)\n",
    "\n",
    "df_tweets_shorten = df_tweets.drop(emp_ind).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some rows have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of rows:\", len(df_tweets.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows that mention a stock: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of rows that mention a stock:\", len(df_tweets_shorten.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts\n",
    "\n",
    "We apply `tweet_to_wordcounts` to each tweet in `df_tweets_shorten[\"text\"]` that mentions at least one stock, either through indices or company names. Then, we store the results in `wordcounts_all`. \n",
    "<br><br>\n",
    "Warning: this step may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_all = [[-1 for i in range(df_tweets_shorten.shape[0])] for j in range(len(keys))]\n",
    "\n",
    "for i in range(df_tweets_shorten.shape[0]):\n",
    "    wordcounts = tweet_to_wordcounts(df_tweets_shorten[\"text\"].iloc[i], keys)\n",
    "    for j in range(len(keys)):\n",
    "        wordcounts_all[j][i] = wordcounts[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we put the resulting word counts for each phrase library into the corresponding new column, e.g. `Word_count_Henry08_pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(keys)):\n",
    "    df_tweets_shorten[\"Word_count_\" + key_library[j]] = wordcounts_all[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Applying all the above operations related to word counts and mentioned stock indices, we modify `df_tweets` to the following form. Note that word counts are normalized, i.e. divided, by the total word count for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Mentioned_stocks_S&amp;P500</th>\n",
       "      <th>Mentioned_stocks_NYSE</th>\n",
       "      <th>Word_count_Henry08_pos</th>\n",
       "      <th>Word_count_Henry08_neg</th>\n",
       "      <th>Word_count_LM11_pos</th>\n",
       "      <th>Word_count_LM11_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BLSP huge volume, closes up 12.5%.  Shares st...</td>\n",
       "      <td>[FB, MSFT, BRK.B, AAPL, TSLA, AMZN]</td>\n",
       "      <td>[HRI]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Polixenes13: Ross, please just never stop ...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kimpaquette I pity the fool that is shorting ...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$TSLA passed 190K model 3 VIN registered. Yeah...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @TeslaCharts: Fraud. Fraud. Fraud. Fraud. \\...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FCC to suspend most operations this week due t...</td>\n",
       "      <td>[AAPL]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://t.co/0HOIOe45er\\n\\n$TSLA news</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Props to @GerberKawasaki for at least spelling...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @stockmarkettv: Tesla Production Numbers So...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bottom Filled on $WDBG Closed up 23% , Ready t...</td>\n",
       "      <td>[FB, MSFT, BRK.B, AAPL, TSLA, AMZN]</td>\n",
       "      <td>[HRI]</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @ElonBachman: Am I hallucinating? $TSLA jus...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Amusingly High quality clip not sure about the...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@GerberKawasaki Congrats on a rockin 3.8% gain...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@Gordon_GekkoZ COUGH $TSLA COUGH\\n\\nWhy couldn...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @evannex_com: Does it make sense to pull th...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   $BLSP huge volume, closes up 12.5%.  Shares st...   \n",
       "1   RT @Polixenes13: Ross, please just never stop ...   \n",
       "2   @kimpaquette I pity the fool that is shorting ...   \n",
       "5   $TSLA passed 190K model 3 VIN registered. Yeah...   \n",
       "7   RT @TeslaCharts: Fraud. Fraud. Fraud. Fraud. \\...   \n",
       "8   FCC to suspend most operations this week due t...   \n",
       "9               https://t.co/0HOIOe45er\\n\\n$TSLA news   \n",
       "11  Props to @GerberKawasaki for at least spelling...   \n",
       "12  RT @stockmarkettv: Tesla Production Numbers So...   \n",
       "15  Bottom Filled on $WDBG Closed up 23% , Ready t...   \n",
       "16  RT @ElonBachman: Am I hallucinating? $TSLA jus...   \n",
       "25  Amusingly High quality clip not sure about the...   \n",
       "26  @GerberKawasaki Congrats on a rockin 3.8% gain...   \n",
       "27  @Gordon_GekkoZ COUGH $TSLA COUGH\\n\\nWhy couldn...   \n",
       "29  RT @evannex_com: Does it make sense to pull th...   \n",
       "\n",
       "                Mentioned_stocks_S&P500 Mentioned_stocks_NYSE  \\\n",
       "0   [FB, MSFT, BRK.B, AAPL, TSLA, AMZN]                 [HRI]   \n",
       "1                                [TSLA]                    []   \n",
       "2                                [TSLA]                    []   \n",
       "5                                [TSLA]                    []   \n",
       "7                                [TSLA]                    []   \n",
       "8                                [AAPL]                    []   \n",
       "9                                [TSLA]                    []   \n",
       "11                               [TSLA]                    []   \n",
       "12                               [TSLA]                    []   \n",
       "15  [FB, MSFT, BRK.B, AAPL, TSLA, AMZN]                 [HRI]   \n",
       "16                               [TSLA]                    []   \n",
       "25                               [TSLA]                    []   \n",
       "26                               [TSLA]                    []   \n",
       "27                               [TSLA]                    []   \n",
       "29                               [TSLA]                    []   \n",
       "\n",
       "    Word_count_Henry08_pos  Word_count_Henry08_neg  Word_count_LM11_pos  \\\n",
       "0                 0.000000                     0.0             0.000000   \n",
       "1                 0.000000                     0.0             0.000000   \n",
       "2                 0.000000                     0.0             0.000000   \n",
       "5                 0.000000                     0.0             0.000000   \n",
       "7                 0.000000                     0.0             0.000000   \n",
       "8                 0.000000                     0.0             0.000000   \n",
       "9                 0.000000                     0.0             0.000000   \n",
       "11                0.000000                     0.0             0.000000   \n",
       "12                0.000000                     0.0             0.000000   \n",
       "15                0.003322                     0.0             0.003322   \n",
       "16                0.000000                     0.0             0.000000   \n",
       "25                0.010638                     0.0             0.010638   \n",
       "26                0.000000                     0.0             0.010101   \n",
       "27                0.000000                     0.0             0.000000   \n",
       "29                0.000000                     0.0             0.000000   \n",
       "\n",
       "    Word_count_LM11_neg  \n",
       "0              0.000000  \n",
       "1              0.014286  \n",
       "2              0.000000  \n",
       "5              0.000000  \n",
       "7              0.053333  \n",
       "8              0.016529  \n",
       "9              0.000000  \n",
       "11             0.000000  \n",
       "12             0.000000  \n",
       "15             0.000000  \n",
       "16             0.000000  \n",
       "25             0.000000  \n",
       "26             0.000000  \n",
       "27             0.000000  \n",
       "29             0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_shorten[[\"text\", \"Mentioned_stocks_S&P500\", \"Mentioned_stocks_NYSE\", \"Word_count_Henry08_pos\",\n",
    "                  \"Word_count_Henry08_neg\", \"Word_count_LM11_pos\", \"Word_count_LM11_neg\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the note on top of this notebook about my remarks on stock indices.\n",
    "<br><br>\n",
    "Finally, we save the new `df_tweets` onto a new csv file called `tweets_data_features_added.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "#df_tweets_shorten.to_csv(localpath + \"/data/Tweets_Preprocessed/tweets_data_features_added.csv\", index=False)\n",
    "\n",
    "# Run this during the trial run\n",
    "df_tweets_shorten.to_csv(localpath + \"/data/Tweets_Preprocessed/df_tsla_aapl_features_added.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>\n",
    "### Helper Functions\n",
    "\n",
    "For brevity, we write down all the necessary but lengthy functions in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains keywords separated by space.\n",
    "                \n",
    "    Output: \n",
    "    keywords_lemm -> The list of strings, each of which is a lemmatized word from the txt file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define keywords\n",
    "    keywords = []\n",
    "    \n",
    "    # This is to keep track of the word we are reading as we traverse text.\n",
    "    this_word = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":    # When running into \" \", we have finished reading a word. \n",
    "            keywords.append(this_word)\n",
    "            this_word = \"\"\n",
    "        elif text[i:] == \"\\n\":   # This may occur at the end of the string.\n",
    "            break\n",
    "        else:     # With an additional letter, just add it to the current word.\n",
    "            this_word = this_word + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \" \", we will need to append the last word to keywords.\n",
    "    if this_word != \"\":\n",
    "        keywords.append(this_word)\n",
    "    \n",
    "    # Lemmatize keywords. See the function below.\n",
    "    keywords_lemm_rept = key_lemmatize(keywords)\n",
    "    \n",
    "    # keywords_lemm_rept likely contains repeated elements due to words that stem from the same root. Eliminate them.\n",
    "    keywords_lemm = list(set(keywords_lemm_rept))\n",
    "    \n",
    "    return keywords_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keybigrams(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains key bigrams separated by \"\\n\". \n",
    "    \n",
    "    This function should work for any n-grams, given that the text file is written in the same format.\n",
    "                \n",
    "    Output: \n",
    "    keybigrams_lemm -> The list of strings, each of which is a lemmatized bigram from the csv file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define keywords\n",
    "    keybigrams = []\n",
    "    \n",
    "    # This is to keep track of the word we are reading as we traverse text.\n",
    "    this_bigram = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \"\\n\":    # When running into \"\\n\", we have finished reading a bigram.\n",
    "            keybigrams.append(this_bigram)\n",
    "            this_bigram = \"\"\n",
    "        else:     # With an additional letter or space, just add it to the current bigram.\n",
    "            this_bigram = this_bigram + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \"\\n\", we will need to append the last bigram to keybigrams.\n",
    "    if this_bigram != \"\":\n",
    "        keybigrams.append(this_bigram)\n",
    "    \n",
    "    # Lemmatize keybigrams. See the function below.\n",
    "    keybigrams_lemm_rept = key_lemmatize(keybigrams)\n",
    "    \n",
    "    # keybigrams_lemm_rept likely contains repeated elements. Eliminate them.\n",
    "    keybigrams_lemm = list(set(keybigrams_lemm_rept))\n",
    "    \n",
    "    return keybigrams_lemm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_lemmatize(keywords):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    keywords -> The list of key words/bigrams extracted from a library file.\n",
    "    \n",
    "    Output:\n",
    "    keywords_lemm -> A list with elements from keywords, each converted into its lemma form using spaCy\n",
    "    \"\"\"\n",
    "    # Define keywords_lemm\n",
    "    keywords_lemm = []\n",
    "    \n",
    "    # Populate keywords_lemm by strings written from the lemma of the word.\n",
    "    for i in range(len(keywords)):\n",
    "        this_doc = nlp(keywords[i])\n",
    "        this_word_lemm = \"\"\n",
    "        for token in this_doc:\n",
    "            this_word_lemm = this_word_lemm + token.lemma_ + \" \"   # Add space in case there are multiple words\n",
    "        keywords_lemm.append(this_word_lemm[:-1])      # Discard the final space\n",
    "    \n",
    "    return keywords_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    \n",
    "    Output:\n",
    "    processed_doc -> A spaCy's doc object built from tweet with stop words (do, is, not, you, etc) removed.\n",
    "    \"\"\"\n",
    "    # Convert tweet into doc\n",
    "    doc_raw = nlp(tweet)\n",
    "    \n",
    "    # Define and populate the list of all tokens in doc_raw\n",
    "    token_list = []\n",
    "    for token in doc_raw:\n",
    "        token_list.append(token)\n",
    "    \n",
    "    # Define and write down the tweet without stop words\n",
    "    tweet_cleaned = \"\"\n",
    "    for token in token_list:\n",
    "        if not token.is_stop:\n",
    "            tweet_cleaned = tweet_cleaned + token.text + \" \"\n",
    "    \n",
    "    # Finally, convert to doc once again\n",
    "    return nlp(tweet_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_wordlocs(tweet, keys, remove_stop=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    keys -> The list of lists of key words/bigrams. For example, keys = [henry08_pos, henry08_neg, ..., newslib]\n",
    "            Each key word/bigram is assumed to contain only English letter and space.\n",
    "    remove_stop -> If True, remove stop words (do, is, not, you, etc) from tweet as a preprocessing step.\n",
    "                   This option is recommended if the user would like to use the bigram lists by Hagenau et al (2013)\n",
    "                   because they come with stop words removed, e.g. \"able add\" assumes that \"to\" has been removed.\n",
    "    \n",
    "    Output:\n",
    "    wordlocs -> A list whose elements are of the form [keyloc, start, end], such that for the key word/bigram \n",
    "    from the i-th library, keys[i], located in tweet from tweet[j] to tweet[j+k-1] inclusive, we have \n",
    "    keyloc = i, start = j and end = j+k. The elements of wordlocs are sorted based on the value of start.\n",
    "    \"\"\"\n",
    "    # Define the case-insensitive phrase matcher.\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    \n",
    "    # Convert tweet into a spaCy's doc object with stop words removed if called for.\n",
    "    if remove_stop:\n",
    "        tweet_text = preprocess_tweet(tweet)\n",
    "    else:\n",
    "        tweet_text = nlp(tweet)\n",
    "    \n",
    "    # Tokenize the key phrases and introduce them to the phrase matcher model.\n",
    "    for i in range(len(keys)):\n",
    "        phrases = [nlp(phrase) for phrase in keys[i]]\n",
    "        matcher.add(str(i), phrases)\n",
    "    \n",
    "    # Find the matches\n",
    "    matches = matcher(tweet_text)\n",
    "    \n",
    "    # Define and populate wordlocs\n",
    "    wordlocs = []\n",
    "    for i in range(len(matches)):\n",
    "        keyloc_shifted, start, end = matches[i]\n",
    "        keyloc = int(nlp.vocab.strings[keyloc_shifted])\n",
    "        wordlocs.append([keyloc, start, end])\n",
    "    \n",
    "    return wordlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlocs_to_wordcounts(wordlocs, tweet_length, num_keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    wordlocs -> The list of keys and locations found in the tweet, c.f. tweet_to_wordlocs function.\n",
    "    tweet_length -> The length of tweet in words.\n",
    "    num_keys -> The number of phrase lists, i.e. len(keys) from tweet_to_wordlocs function.\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define and populate wordcounts\n",
    "    wordcounts = [0 for i in range(num_keys)]\n",
    "    for j in range(len(wordlocs)):\n",
    "        wordcounts[wordlocs[j][0]] += 1\n",
    "    \n",
    "    # Perform normalization as needed\n",
    "    if normalize:\n",
    "        return [wordcounts[i]/tweet_length for i in range(num_keys)]\n",
    "    \n",
    "    # In the case where normalization is not called for\n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_wordcounts(tweet, keys, remove_stop=True, normalize=True):\n",
    "    \"\"\"\n",
    "    This function performs tweet_to_wordlocs then plug everything into wordlocs_to_wordcounts.\n",
    "    \"\"\"\n",
    "    wordlocs = tweet_to_wordlocs(tweet, keys, remove_stop)\n",
    "    wordcounts = wordlocs_to_wordcounts(wordlocs, len(tweet), len(keys), normalize=True)\n",
    "    \n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_list(tweet, stock_indices, company_names=[]):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    stock_indices -> The list of stock indices in string\n",
    "    company_names -> The list of company names corresponding to stock_indices\n",
    "                     If not provided, company names will not be searched for in tweet.\n",
    "    \n",
    "    Output:\n",
    "    stock_list -> A list of stock indices in stock_indices that are mentioned in tweet,\n",
    "                  either by indices or by company names.\n",
    "    \"\"\"\n",
    "    # To make this case-insensitive, make tweet all lowercase.\n",
    "    tweet_processed = tweet.lower()\n",
    "    \n",
    "    # Initialize stock_list as an empty list.\n",
    "    stock_list = []\n",
    "    \n",
    "    # For each stock index, make it lowercase then find if it appears in tweet_processed.\n",
    "    # To avoid false positives (in finding a mention), we only consider indices followed by \" \"\n",
    "    # and preceeded by \"$\" or \"#\"\n",
    "    for i in range(len(stock_indices)):\n",
    "        loc_dollar = tweet_processed.find(\"$\" + stock_indices[i].lower() + \" \")\n",
    "        loc_hashtag = tweet_processed.find(\"#\" + stock_indices[i].lower() + \" \")\n",
    "        if max(loc_dollar, loc_hashtag) >= 0:\n",
    "            stock_list.append(stock_indices[i])\n",
    "    \n",
    "    # For each company name, if available, make it lowercase then find if it appears in tweet_processed.\n",
    "    for i in range(len(company_names)):\n",
    "        loc = tweet_processed.find(\" \" + company_names[i].lower() + \" \")\n",
    "        if loc >= 0:\n",
    "            stock_list.append(stock_indices[i])\n",
    "    \n",
    "    return list(set(stock_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
